---
output:
  html_document: default
  pdf_document: default
---

# Week 12: Categorical predictors

> Written by Margriet Groen (partly adapted from materials developed by the PsyTeachR team a the University of Glasgow)

So far, all the predictors in the models we’ve looked at were continuous variables. What if you wanted to know whether a response differed between two or more discrete groups? Hang on, you might say, that sounds like doing an ANOVA. True, you might have used ANOVA to assess whether group means differed in previous stats courses. ANOVAs—to some degree—are just a special type of regression where you have categorical predictors. This week we’ll look at how to model responses as a function of categorical predictors and we’ll combine categorical predictors to model how a predictor might affect the outcome variable differently across two different groups. For example, we might be interested in whether the amount of time adolescents use digital devices (screen-time) predicts their well-being. Additionally, we might want to know whether well-being is different for adolescent boys and girls and whether the relationship between screen-time and well-being differs for these two groups. By fitting a regression model in which we combine a continuous (screen-time) and a categorical (sex) predictor, we can do exactly that. We’ll be working on that in the lab.

## Lectures
The lecture material for this week follows the recommended chapters in Winter (2020) -- see under 'Reading' below -- and is presented in two parts:

* [**Categorical predictors (~17 min)**](https://web.microsoftstream.com/video/5c263506-1ebc-4c54-b4aa-2cab8de826d6)

* [**Interactions (~18 min)**](https://web.microsoftstream.com/video/9c3b7a03-dc41-47d9-b8c5-0835ea1da9d4) 

## Reading

### Blogpost by Professor Dorothy Bishop
In this [**very short blogpost**](https://deevybee.blogspot.com/2017/11/anova-t-tests-and-regression-different.html?m=1) Professor Dorothy Bishop explains the links between ANOVA and Regression.

### Winter (2020)
[**Link**](https://eu.alma.exlibrisgroup.com/leganto/public/44LAN_INST/citation/83408786230001221?auth=SAML)

**Chapter 7** provides an excellent overview of using categorical predictors in regression models and explains how this is implemented in R.

**Chapter 8** explains what interactions are and how to model and interpret them.

## Pre-lab activities
After having watched the lectures and read the textbook chapters you’ll be in a good position to try these activities. Completing them before you attend your lab session will help you to consolidate your learning and help move through the lab activities more smoothly.

### Pre-lab activity 1: Data-wrangling in R
The more you practise coding in R, the easier it will become. The RStudio interactive tutorials I mentioned last week are an excellent place to start if you haven't engaged with those yet. 

* [**The Basics**](https://rstudio.cloud/learn/primers/1) Start here to learn how to inspect, visualize, subset and transform your data, as well as how to run code.

* [**Work with Data**](https://rstudio.cloud/learn/primers/2) Learn how to extract values form a table, subset tables, calculate summary statistics, and derive new variables.

* [**Visualize Data**](https://rstudio.cloud/learn/primers/3) Learn how to use ggplot2 to make any type of plot with your data. The tutorials on [**Exploratory Data Analysis**](https://rstudio.cloud/learn/primers/3.1) and [**Scatterplots**](https://rstudio.cloud/learn/primers/3.5) are particularly relevant.

If you feel confident with the material covered in those tutorials the following are useful to try:

* [**Separating and Uniting Columns**](https://rstudio.cloud/learn/primers/4.2) Here you will learn to separate a column into multiple columns and to reverse the process by uniting multiple columns into a single column. Then you'll practise your data wrangling skills on messy real world data.

* [**Join Data Sets**](https://rstudio.cloud/learn/primers/4.3) Learn how to work with relational data. Here you will learn how to augment data sets with information from related data sets, as well as how to filter one data set against another.

Please note that there are often different ways to do the same or similar things in R. This means you might encounter slightly different functions or styles of coding in different materials. This is not something to worry about. Just make sure you're clear on what a bit of code achieves and choose the function/style that you feel most comfortable with.

### Pre-lab activity 2: Getting ready for the lab class
#### Remind yourself of how to access and work with the RStudio Server.

* [**Video on how to access the RStudio Server by Padraic**](https://dtu-panopto.lancs.ac.uk/Panopto/Pages/Viewer.aspx?id=f4f414ff-dd5b-4301-9214-adbf009d10da)
* I highly recommend using R Projects to structure your workflow. You could create an R project for each week of the module. Have a look at section [**8 Workflow: projects**](https://r4ds.had.co.nz/workflow-projects.html) of **R for Data Science** by Hadley Wickam and Gareth Grolemund for an introduction.

#### Get your files ready
Download the [402_week12_forStudents.zip](files/week12/402_week12_forStudents.zip) file and upload it into a new folder in RStudio Server. 

## Lab activities
In this lab, you’ll gain understanding of and practice with:

* when and why to apply multiple regression to answer questions in psychological science
* conducting multiple regression in R when combining continuous and categorical predictors
* interpreting the R output of multiple linear regression (when combining continuous and categorical predictors)
* reporting results for multiple linear regression (when combining continuous and categorical predictors), following APA guidelines

### Lab activity 1: Combining a continuous and a categorical predictor in a resssion model

#### Background: Smartphone screen-time and well-being
There is currently much debate (and hype) surrounding smartphones and their effects on well-being, especially with regard to children and teenagers. We’ll be looking at data from this recent study of English adolescents: Przybylski, A. & Weinstein, N. (2017). A Large-Scale Test of the Goldilocks Hypothesis. *Psychological Science, 28*, 204–215.

This was a large-scale study that found support for the “Goldilocks” hypothesis among adolescents: that there is a “just right” amount of screen-time, such that any amount more or less than this amount is associated with lower well-being. This was a huge survey study with data containing responses from over 120,000 participants! Fortunately, the authors made the data from this study openly available, which allows us to dig deeper into their results. And the question we want to expand on in this lab is whether the relationship between screen-time and well-being depends on the partcipant’s (self-reported) sex. In other words, our research question is: **Does screen-time have a bigger impact on boys or girls, or is it the same for both?**

The dependent measure used in the study was the [**Warwick-Edinburgh Mental Well-Being Scale (WEMWBS)**](https://warwick.ac.uk/fac/sci/med/research/platform/wemwbs). This is a 14-item scale with 5 response categories, summed together to form a single score ranging from 14-70.

On [**Przybylski & Weinstein’s page for this study on the Open Science Framework**](https://osf.io/82ybd/), you can find the participant survey, which asks a large number of additional questions (see page 14 for the WEMWBS questions and pages 4-5 for the questions about screen-time). Within the same page you can also find the [**raw data**](https://osf.io/82ybd/), which some of you might want to consider using for your research report.

However, for the purpose of this lab, you will be using local pre-processed copies of the data (`participant_info.csv`, `screen_time.csv` and `wellbeing.csv, which you downloaded as part of the 'Pre-lab activities'.

Przybylski and Weinstein looked at multiple measures of screen-time, but again for the interests of this lab we will be focusing on smartphone use, but do feel free to expand your skills after by looking at different definitions of screen-time. Overall, Przybylski and Weinstein suggested that decrements in well-being started to appear when respondents reported more than one hour of daily smartphone use. So, bringing it back to our additional variable of sex, **our research question is now: Does the negative association between hours of smartphone use and well-being (beyond the one-hour point) differ for boys and girls?**

Let’s think about this in terms of the variables. We have:

*	a continuous outcome variable: well-being;
*	a continuous∗ predictor variable: screen-time;
*	a categorical predictor variable: sex.

Please note that well-being and screen-time are technically only **quasi-continuous** inasmuch as that only discrete values are possible. However, there are a sufficient number of discrete categories in our data that we can treat the data as effectively continuous.

Now, in terms of analysis, what we are effectively trying to do is to estimate **two slopes** relating screen-time to well-being, one for adolescent girls and one for adolescent boys, and then statistically compare these slopes. Sort of like running a correlation for boys, a correlation for girls, and comparing the two. Or alternatively, where you would run a regression (to estimate the slopes) but also one where you would need a t-test (to compare two groups). But the expressive power of regression allows us to do this all within a single model. Again, as we have seen building up to this lab, an independent groups t-test is just a special case of ordinary regression with a single categorical predictor; ANOVA is just a special case of regression where all predictors are categorical. But remember, although we can express any ANOVA design using regression, the converse is not true: we cannot express every regression design in ANOVA. As such people like regression, and the general linear model, as it allows us to have any combination of continuous and categorical predictors in the model. The only inconvenience with running ANOVA models as regression models is that you have to take care in how you numerically code the categorical predictors. We will use an approach called **deviation coding** which we will look at today later in this lab.

To complete this lab activity, you can use the R-script (`402_wk12_labAct1_template.R`) that you downloaded as part of the 'Pre-lab activities' as a template. Work through the activity below, adding relevant bits of code to your script as you go along.

#### Step 1: Background and set up

Before we get stuck in we need to set up a few things. 

> **TASK**: Add code to clear the environment. **HINT**: `rm(list=ls())`

Next we need to tell R which libraries to use. We need `broom`, `car` and `tidyverse`.

> **TASK**: Add code to load relevant libraries. **HINT**: `library()`

Finally, read in the three data files; call the participant info `pinfo`; call the screen_time data `screen` and the well-being data `wellbeing`.

> **TASK*: Add code to read in the three data files. **HINT**: Use the `read_csv()` function.

#### Step 2: Checking the formatting
Given our research question and the information you have about the scores,
provided above under 'Background' and from the OSF-webpage, is the data ready for use?

> **TASK**: Add code to look at the first few lines of each data frame. **HINT**: Use the `head()` function (or `tail()` function).

**QUESTION 1**: In which table is the variable corresponding to sex located and what is this variable called?
 
The 'source and analysis code.sps' file in the 'Data and Code' section on the [**OSF-webpage**](https://osf.io/82ybd/) tells us how they coded the sex variable:  0 = female indicator and 1 = male indicator. It is worth exploring the OSF-webpage, to get used to foraging other files for these kinds of information, as they are not always clearly explained in a codebook or README. file.

For ease, lets recode the sex variable to reflect word labels of 'female' and 'male'. This doesn't change the order: R will still see female as 0, and male as 1 because female occurs before male in the alphabet. Add the code below to your script to do this. Don't forget to run it as well.


```{r eval=FALSE}
pinfo$sex <- ifelse(pinfo$sex == 1, "male", "female")
head(pinfo)                            
```

**QUESTION 2**: In what format is the well-being data (long or wide)? On how many participants does it include observations? And on how many items for each participant?

**QUESTION 3**: What is the name of the variable that identifies individual participants in this dataset? It is important to work this out as this variable will allow us to link information across the three data files.

#### Step 3: Data preparation - Aggregating the total well-being scores 
We need to sum the 14 items of the well-being scale for each participant to create one well-being score per participant.

> **TASK**: To create one well-being score per participant add code to the script to do the following: first, transform the `well-being` variable from wide to long (using `pivot_longer()`); then, use `group_by()` to get scores for each participant and finally use `summarise()` to calculate a total well-being score, calling the new variable `tot_wellbeing`. Save all of this to an object called 'wb_tot'.

It is useful to calculate some descriptive statistics for the new variable `tot_wellbeing`.

> **TASK**: Calculate some descriptive statistics for `tot_wellbeing`. **HINT**: Use `summarise()` to calculate the mean, standard deviation, minimum and maximum values.

Finally, let's get an idea of the distribution of the new variable `tot_wellbeing`.

> **TASK**: Visualise the distribution in a histogram. **HINT**: Use `ggplot()` and `geom_historgram()`.

**QUESTION 4**: Is the distribution of well-being scores symmetrical, negatively skewed or positively skewed?

#### Step 4: Data preparation - Transforming screen time data
Great, so we have the well-being scores sorted out, we now need to think about the screen-time usage data and
whether it is being used on a weekday or a weekend. As always, to get an idea of the data, it is often very
useful to visualise the variables before proceeding with the analysis.

Before we can do this, we'll need to tidy these data up. Have another look at the `screen` data by using the `head()` function. You'll see that we have `Serial` in the first column (this is good), but in the following eight columns, we have columns for each type of activity (Comph, Comp, Smart, Watch) and the part of the week it took place (we and wk) combined. Instead, to be able to work with the data, we need two columns: one for the type of activity (we'll call it `variable`) and one for the part of the week (we'll call it `day`).

A second issue is that we need to alter the abbreviations Comph, Comp, Smart and Watch to reflect more descriptive text for each in plots.

Below are two chunks of code that represent these steps. In the next tasks you'll practise with taking a set of piped commands apart. The purpose of this is to get you used to "parsing" the code at the right places so that when you see piped commands in other people's code, you know how to break it down and find the relevant parts that you can use.

> **TASK**: In the code chunk below we use the `separate()` function to split the character strings already in the dataset. You know that with piped commands, there are chunks of code. Run the code first in its entirety and then pull each line apart to see how each function works on the data. Write a descriptive sentence for each function's role in the command. Don't forget to copy the chunk to your script and run it.

```{r eval=FALSE}
screen_long <- screen %>%
  pivot_longer(-Serial, names_to = "var", values_to = "hours") %>%
  separate(var, c("variable", "day"), "_")                       
```

> **TASK**: In the next code chunk we use the `dplyr::recode()` function with `mutate()` to relabel the separated names into
understandable names that will be clear in plots. Again, run the code first in its entirety and then pull each line apart to see how each function works on the data. Write a descriptive sentence for each function's role in the command. Don't forget to copy the chunk to your script and run it.

```{r eval=FALSE}
screen2 <- screen_long %>%
  mutate(variable = dplyr::recode(variable,
                                  "Watch" = "Watching TV",
                                  "Comp" = "Playing Video Games",
                                  "Comph" = "Using Computers",
                                  "Smart" = "Using Smartphone"),
         day = dplyr::recode(day,
                             "wk" = "Weekday",
                             "we" = "Weekend"))                   
```

The code above has a new feature: the `dplyr::recode` part. This syntax -- using the double colon -- happens when there are many versions of a function with the same name. You can imagine that a function called 'recode' is immensely useful at the data wrangling stage of analysis. By using the name of the package, a double set of colons, followed by a function name, you are ensuring that R uses a particular version of the function, at that point only. This avoids having two or more packages loaded in your environment that sometimes do not play nicely together!

To be able to monitor that your code is performing as you want it to, you need to have in your mind an idea of how the data should look at the end of a code chunk. So stop a moment and be clear, discuss with your lab-mates if you feel like it and answer the following question.

**QUESTION 5**: What are the variables and the levels or conditions within each variable of screen2?

>**TASK**:Now join `wb_tot` and `screen_2` by participant and then group by the variables 'variable', 'day' and 'hours' and then calculate a 'mean_wellbeing' variable for each of the grouped levels. Save it in an object called 'dat_means'.**HINT**: Write separate lines of code for each action and then, when you know they all work, reformat them as a piped command; look back earlier in this code and last week for the functions that perform these actions.

> **TASK**: Now check that you have an object that is 72 observations of 4 variables. You should have a mean wellbeing score for every level of the hours, over weekdays and weekends for each level of the four types of screen time (4 x 2 x 9)

Next, it is a good idea to visualise the mean well-being data as function of hours of screen-time for the different days (weekday vs. weekend) and types of screen (playing video games, using computers, using smartphone and watching tv). This is quite a complex graph. We'll go through creating it step-by-step, but let's first look at the end result:

![](files/week12/wellbeingByScreentime.png)

Ok, that's what we are working towards.

> **TASK**: Below, a chunk of code is presented. It is your task to fill in the x and y variables. **HINT**: Go back to the research question - which variable is for the x axis and for the y axis?

```{r eval=FALSE}
ggplot(dat_means, aes(x = , y = )) +
  geom_line() +
  geom_point() +
  theme_bw()
```

**QUESTION 6a**: What research question does this plot describe? Is it appropriate for the levels within the data?

> **TASK**: Now, let's add a different linetype for each day (weekday vs. weekend). Fill in the blanks in the code below.

```{r eval=FALSE}
ggplot(dat_means, aes(x = , y = , linetype = )) +
  geom_line() +
  geom_point() +
  theme_bw()
```

**QUESTION 6b**: What research question does this plot describe? Is it appropriate for the levels within the data?

Still not quite there.

>**TASK**: Fill in the blanks (for x, y and linetype) as before. Now have a good look at the code below. What has changed? Copy the code to your script and run it. Then, for each line write a sentence as a comment to describe its effect on the plot.

```{r eval=FALSE}
ggplot(dat_means, aes(x = , y = , linetype = )) +
  geom_line() +
  geom_point() +
  facet_wrap(~variable, nrow = 2) +
  theme_bw()
```

We add the `facet_wrap()` function here. You can check the `?facet_wrap()` help page for more information.

**QUESTION 6**: c. What does the `facet_wrap()` function do? Is this plot appropriate for the levels in the data?

#### Step 5: Calculating mean hours per day for smartphone use, for each participant
As mentioned at the beginning, in today's lab we'll focus on smartphone use. So looking at the bottom left of the figure we could suggest that smartphone use of more than 1 hour per day is associated with increasingly negative well-being the longer screen time people have. This looks to be a similar effect for Weekdays and Weekends, though perhaps overall well-being in Weekdays is marginally lower than in Weekends (the line for Weekday is lower on the y-axis than Weekends). This makes some sense as people tend to be happier on Weekends!

> **TASK**: Below is a set of comments that describe what the chunk of code that you need to write next does:
use 'screen2'
and then filter out the observations for 'Using Smartphone',
and then group together each participant,
and then summarise the mean hours calling it 'hours_per_day',
save it in an object called 'smarttot'

> **TASK**: Now let's do it the other way around. Run the code below. Have a look at the structure of 'smart_wb' **HINT**: You can use the `str()` function. What does the code do? 

```{r eval=FALSE}
smart_wb <- smarttot %>%
  filter(hours_per_day > 1) %>%
  inner_join(wb_tot, "Serial") %>%
  inner_join(pinfo, "Serial") %>%
  mutate(sex = as.factor(sex)) 
```

**QUESTION 7**: What does the code do? Write a short paragraph, using the phrase "and then" to represent the pipes.

#### Step 6: More visualisation
We are now using only one small part of the data - smartphone use and its relationship with well-being over different durations of time. Before formally testing our research question, we can visualise the data and enquire about sex differences on the same plot - run each chunk of code below:

> **TASK** To further group the data, copy the code below to your script and run it. Look at the 'smart_wb_gen' dataframe. What has the code above done? Write a couple of sentences of description

```{r eval=FALSE}
smart_wb_gen <- smart_wb %>%
  group_by(hours_per_day, sex) %>%
  summarise(mean_wellbeing = mean(tot_wellbeing))
```

> **TASK**: Let's visualise these data.

```{r eval=FALSE}
ggplot(smart_wb_gen, aes(hours_per_day, mean_wellbeing, color = sex)) +
  geom_point() +
  geom_smooth(method = "lm") +
  scale_color_discrete(name = "Sex", labels = c("Girls", "Boys"))+
  scale_x_continuous(name = "Total hours smartphone use") +
  scale_y_continuous(name = "Mean well-being score") +
  theme_bw()
```

**QUESTION 8**: Write an interpretation of the above plot in plain English.

#### Step 7: The regression model

In the steps 2 to 6 we've covered some pretty heavy-lifting data-wrangling. As it is so often the case that something like this is needed when working with real data, ti is really important to practise this. However, to ensure you also spend time on fitting the regression model and interpreting the output, you can choose to use the data-file [smart_wb.csv](files/week12/smart_wb.csv) to get started with that. It contains the data in a format that is the result of all the data-wrangling we did in steps 2 to 6. So, download the smart_sb.csv data-file, put it in the folder that is your working directory and you're all set for running the regression model.

> **TASK**: Let's run the regression. Write code in your script in which you call your output 'mod', and use the data 'smart_wb' using the following formula, `lm(y ~ x1 + x2 + x1:x2, data)` to construct your regression model. Go back to the research question for your outcome and two predictor variables.

> **TASK**: Call and save the summary of your model as 'mod_summary'; then have a look at it.

Let's first look at the model as a whole:

**QUESTION 9**: What is the p-value for the overall model? Is it significant? What does this mean?

**QUESTION 10**: To two decimal places, what percentage of the variance in well-being scores does the overall model explain?

Now, lets look at the coefficients of our predictors:

**QUESTION 11**: Are the main effects of smartphone use and sex significant?

**QUESTION 12**: Which variable indicates the interaction between smartphone use and sex?

**QUESTION 13**: And is the interaction significant?

**QUESTION 14**: What is the most reasonable interpretation of these results? 

The above model uses **treatment coding** (sometimes called dummy coding), for the sex variable. In a categorical variable with only two levels this means that one level is coded as 0 and the other level is coded as 1. In categorical variables with more than two levels, it works slightly differently.

> **TASK**: We can check that the sex variable is treatment or dummy coded with the following code:

```{r eval=FALSE}
contrasts(smart_wb$sex)
```

Because we have not explicitly told R about the labels for the sex variable, it has used level 0 as the reference level, hidden within the intercept term and level sex1 describes the difference (or slope) between level 0 and 1 or in this dataset from female to male.

**QUESTION 15**: Is being Male better for a person's well-being in the context of smartphone use than being Female?

Now let's look at **deviation coding**. There are other ways to code your categorical variables. One of them is deviation coding (also sometimes called sum coding). This effectively divides the difference of the values between your categorical levels by the number of levels so that each level can be compared to one intercept that is central to them all rather than comparing levels to one reference level. It is like centering for a categorical level.

> **TASK** Use the code chunk below to: 1) add a variable to the smart_wb data that is a deviation coding of sex; 2) set the deviation coding (we'll labe it 'Sum' here for easy variable naming); and 3) look at the output for the sum-coded sex variable.

```{r eval=FALSE}
smart_wb <- mutate(smart_wb, sexSum = sex) # add a variable to the smart_wb data that is a deviation coding of sex
contrasts(smart_wb$sexSum) <- contr.sum(2) # wet the deviation coding
contrasts(smart_wb$sexSum) # look at the output for the sum coded sex variable
```

Next, we'll run the regression again, using the sum-coded sex variable and we'll compare the outputs.

```{r eval=FALSE}
# Run the regression model again, using the sumcoded sex model and compare outputs
mod_S <- lm(tot_wellbeing ~ hours_per_day + sexSum + hours_per_day:sexSum, smart_wb)
mod_S_summary <- summary(mod_S)

# Compare the two model summary outputs
mod_summary
mod_S_summary
```

'sexSum1' is now the coefficient for sex and represents the change from the intercept value which now lies between the values for being Female and Male. Note how this coefficient is a negative.

The earlier model had a positive coefficient because the intercept described the reference group of the Girls, who on average begin at a lower well-being level than Boys (refer back to the scatterplot to verify this). Because the sum-coding has moved the intercept to a point that is the center of the difference between Boys and Girls, sexSum1 now describes the distance between the centre and a level of Sex.

Values for well-being in Girls are thus: Intercept + sexSum*+1
49.74 + (-1.61)*(+1)

Values for well-being in Boys are thus: Intercept + sexSum*-1
49.74 + (-1.61)*(-1)

with the Boys being higher in well-being...(remember a negative number multiplied by a negative number produces a positive number and a negative number multiplied by a positive number produces a negative number).

The interpretation of both model effects is the same, and if you look at the summary statistics, they are identical. Deviation coding effectively centers your categorical variables and helps with interpretation of interaction terms.

#### Step 8: Checking assumptions
Now that we've fit a model, let's check whether it meets the assumptions of linearity, normality and homoscedasticity. With regression models, you do this after you've actually fit the model.

**Linearity**
Unlike when we did simple regression we can’t use `crPlots()` to test for linearity when there is an interaction, but we know from looking at the grouped scatterplot that this assumption has been met.

**Normality**
Normally we would test for normality with a QQ-plot and a Shapiro-Wilk test. However, because this dataset is so large, the Shapiro-Wilk is not appropriate (if you try to run the test it will produce a warning telling you that the sample size must be between 3 and 5000). This is because with extremely large sample sizes the Shapiro-Wilk test will find that any deviation from normality is significant. Therefore we should judge normality based upon the QQ-plot.

> **TASK**: Create a QQ-plot to check the residuals are normally distributed **HINT**: You can use the `qqPlot()` function. The residuals are stored in the 'mod' object you created earlier.

**QUESTION 16**: What do you conclude from the QQ-plot?

**Homoscedasticity**
Here we have the same problem as with testing for normality: with such a large sample the `ncvTest()` will produce a significant result for any deviation from homoscedasticity. So we need to rely on plots again. To check for homoscedasticity we can use `plot()` from Base R that will produce a bunch of helpful plots (more information [**here**:](https://www.r-bloggers.com/2016/01/how-to-detect-heteroscedasticity-and-rectify-it/).

> **TASK**: Copy the code chunk below to your script and run it.

```{r eval=FALSE}
par(mfrow=c(2,2))             # 4 charts in 1 panel
plot(mod)                     # this may take a few seconds to run
```

The residuals vs leverage plot shows a flat red line so, whilst it isn’t perfect, we can assume that with such a large sample size regression is still an appropriate analysis.

**Multi-collinearity**
Finally, lets check for multicollinearity using the `vif()` function. Essentially, this function estimates how much the variance of a coefficient is “inflated” because of linear dependence with other predictors, i.e., that a predictor isn’t actually adding any unique variance to the model, it’s just really strongly related to other predictors. Thankfully, vif is not affected by large samples like the other tests. There are various rules of thumb, but most converge on a VIF of above 2 to 2.5 for any one predictor being problematic.

> **TASK**: Copy the code chunk below to your script and run it.

```{r eval=FALSE}
vif(mod)                      # Check for multi-collinearity
```

**QUESTION 17**: Do any of the predictors show evidence of multicollinearity?
## ANSWER: Yes, male and the interaction do. We'll talk about that more later in the module.


#### Step 9: Write up

**QUESTION 18**: How would you write up the results following APA guidance? You can choose whether you do so for the model using treatment coding or for the model using deviation coding.

## Answers

When you have completed all of the lab content, you may want to check your answers with our completed version of the script for this week. **Remember**, looking at this script (studying/revising it) does not replace the process of working through the lab activities, trying them out for yourself, getting stuck, asking questions, finding solutions, adding your own comments, etc. **Actively engaging** with the material is the way to learn these analysis skills, not by looking at someone else's completed code...

<!-- The answers to the questions and the script containing the code will be available after the lab session has taken place. -->

You can download the R-script that includes the relevant code here: [402_wk12_labAct1_withAnswers.R](files/week12/402_wk12_labAct1_withAnswers.R).

### Lab activity 1: Combining a continuous and a categorical predictor in a resssion model

1. In which table is the variable corresponding to sex located and what is this variable called? **In `pinfo`; the variable is called `sex`.**

2. In what format is the well-being data (long or wide)? On how many participants does it include observations? And on how many items for each participant? **The well-being data are in 'wide' format. It contains observations on 102580 participants, on 14 items.**

3. What is the name of the variable that identifies individual participants in this dataset? It is important to work this out as this variable will allow us to link information across the three data files. **`Serial`**

4. Is the distribution of well-being scores symmetrical, negatively skewed or positively skewed? **Negatively skewed**

5. What are the variables and the levels or conditions within each variable of screen2?

**Participants plus:**
**Levels:  variables = 4:  watching tv, playing video games, using computers, using smartphone**
         **day = 2 = weekdays, weekends**
         **hours = 9 = 0, 0.5, 1 - 7**

6.

+ a. What research question does this plot describe? Is it appropriate for the levels within the data? **The plot describes how hours of use impact upon well-being? No, it is too broad a research question.**

+ b. What research question would fit this visualisation?Is it appropriate for the levels in the data? **The hours are now displayed by weekdays and weekends. How do hours of screen time impact on well-being on weekdays and at the weekend? No, it is still too broad.**

+ c. What does the `facet_wrap()` function do? Is this plot appropriate for the levels in the data? **The `facet_wrap()` function has split the types of screen time and shown how hours of use across weekdays and weekends impact upon well-being. This captures the levels of information within the dataset.**

7. *See the script*

8. Write an interpretation of the above plot in plain English. **Something along the lines of: Adolescent girls show lower overall well-being compared to adolescent boys. In addition, the slope for girls appears more negative than that for boys; the one for boys appears relatively flat. This suggests that the negative association between well-being and smartphone use is stronger for girls.**

9. What is the p-value for the overall model? Is it significant? What does this mean? **The p-value for the overall model fit is < 2.2e-16. This significant. It means that together the predictors describe the variance in well-being better than a model without the predictors (the null model). So knowing something about smartphone use and sex of participants will allow us to predict their well-being to a degree.**

10. To two decimal places, what percentage of the variance in well-being scores does the overall model explain? **9.38%**

11. Are the main effects of smartphone use and sex significant? **Yes.**

12. Which variable indicates the interaction between smartphone use and sex? **The interaction is indicated by the variable hours_per_day:sex.**

13. And is the interaction significant? **Yes.**

14. What is the most reasonable interpretation of these results? **Smartphone use was more negatively associated with well-being for girls than for boys.**

15. Is being Male better for a person's well-being in the context of smartphone use than being Female? **Yes.**

16. What do you conclude from the QQ-plot? **The residuals are normally distributed.**

17. Do any of the predictors show evidence of multicollinearity? **Yes, Boys and the interaction do. We'll talk about that more, later in the module.**

18. Write up

**Treatment / dummy coded model**
Treatment coding was used for categorical predictors with the Girls level acting as the reference group. The results of the regression indicated that the model significantly predicted well-being (*F*(3, 71029) = 2450.89, *p* < .001, Adjusted R2 = 0.09), accounting for 9% of the variance.Total hours of smart phone use was a significant negative predictor of well-being scores (*β* = -0.77, *p* < .001, as was sex (*β* = 3.22, *p* < .001), with girls having lower well-being scores than boys. Importantly, there was a significant interaction between screen time and sex (*β* = 0.45, *p* < .001): smartphone use was more negatively associated with well-being for girls than for boys.

**Deviation / sum coded model**
Deviation coding was used for categorical predictors. The results of the regression indicated that the model significantly predicted well-being (*F*(3, 71029) = 2450.89, *p* < .001, Adjusted R2 = 0.09), accounting for 9% of the variance. Total hours of smart phone use was a significant negative predictor of well-being scores (*β* = -0.55, *p* < .001, as was sex (*β* = -1.61, *p* < .001), with girls having lower well-being scores than boys. Importantly, there was a significant interaction between screen time and sex (*β* = -0.22, *p* < .001), indicating that smartphone use was more negatively associated with well-being for girls than for boys.